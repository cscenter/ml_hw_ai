# Домашнее задание по курсу "Машинное обучение" тема AI

Предлагается реализовать агента для простой многопользовательской игры с другими агентами.
Это вариант многораундовой ультимативной игры.

По всем вопросом связанным с этим заданием можно обрашаться к Алексею Пшеничному. slack:@izhleba  email: izhleba@gmail.com

## Правила игры

Для игры выбирают двух случайных участников. Дальше выбирают в паре участника (proposer),
которому дают возможность сделать ультиматум. Наприме есть 100 долларов/очков/леденцов.
Он (proposer) должен разделить эти плюшки в каком-то соотношении. К примеру, 100 себе и 0 второму участнику (responder).
Или 70 себе и 30 второму участнику. Второй участник (responder) может либо согласиться на этот ультиматум.
Тогда произойдёт сделка, каждый получит в соответствии с предложенным разбиением.
 Либо отказаться. Тогда оба получают 0.

Проводится множество раундов, после которых сравниваются статистики набранных долларов/очков/леденцов
и распределяются места. Набравшему больше всех первое и т.д.

## Метрика успеха

В качестве метрики успешности агента будет считаться [bayesian average](https://en.wikipedia.org/wiki/Bayesian_average), чтобы избежать в песочнице ситуаций, когда агент сыграл две игры случайно получил хороший результат и больше не играет.

Если `gains` - это массив заработков размера `n`, то `score = (C*m + sum(gains))/(C+len(gains))` где `C`- будет равно 18 (возможны корректировки в будущем) `m` - будет равно 50 (средне значение для примитивного агента).

## Использование песочницы

По адресу <https://ultimategame.ml/> находится песочница. Сюда можно отправить решение и посмотреть результаты.
С некоторой периодичностью там проходят игры, на момент написания каждые 15 минут.
Решения принимаются через Docker контейнеры, об этом будет написано ниже.

### Регистрация

 1. Зайдите на сайт https://ultimategame.ml/ нажмите `Login` в правом верхнем углу, замтем `Register`.
 2. Укажите необходимые поля, пожалуйста заполните настоящие имя и фамилию.
 3. На указанный email придет писмьо с активационной сылкой, пройдиет по ней.
 4. После активации можеет заходит со своим логином и паролем.

### Отправка решения

1. Зайдите в `Submissions` и кликните на `Make submission`
2. В открывшейся форме в поле `Dockerhub image` вставьте адрес вашего Docker образа. Адрес указывается без http префикса и выглядит
как `<username>/<image_name>:<tag>`. О способе создания докер образа смотри инструкцию ниже.
3. В очередной игре участвует самое последнее отправленное решение. Возможно позже появится возможность управлять этим.
4. Список своих отправленных решений можно увидеть пройдя во вкладку `Submissions list`.
5. Поле `Pull Status` показывает на каком этапе сейчас находится решение.
    - 'Waiting' - ожидается загрузка при следующей игре;
    - 'Downloaded' - образ успешно загружен;
    - 'Downloading error' - не удалось загрузить образ с указанного адреса в dockerhub;
    - 'Exited to fast' - контейнер завершил работу сразу после старта;
    - 'Wrong launch' - контейнер запустился, но работает не корректно;
    - 'Ok' - контейнер корректно отработал игру.
6. После успешного участия в очередной игре в поле `Last Score` будет сохранено послежне значение score.
7. Общую таблицу по последней игре можно найти во вкладке `Leaderboard` или на главной странице.

## Создание своего агента

### Простой путь
Для вашего удобства создан класс в котором вам нужно написать код агента `agent.my_agent.MyAgent`.
Необходимо чтобы метод `offer_action` возвращял размер предложения для другого агента, а `deal_action` результат согласия или не согласия.
Метод `round_result_action` позволяет узнать результаты раунда.
Более подробное описание методов и структур данных будет описанно ниже.

Код шаблона агента:
```python
from agent.base import BaseAgent
from base.protocol import OfferRequest, DealRequest, RoundResult

class MyAgent(BaseAgent):

    def offer_action(self, m: OfferRequest) -> int:
        return 1

    def deal_action(self, m: DealRequest) -> bool:
        return True

    def round_result_action(self, data: RoundResult):
        pass
```

### Структура класса
Любой агент, должен наследовать методы от базового класса `agent.base.BaseAgent`.
Далее представленно описание этих методов:
- `def offer_action(self, data)` возващяет размер вашего предложения (int) некоторому агенту в раунде.
Данные об агенте и общем размере разделяемых средства находятся в `data`
структура класса описана здесь `base.protocol.OfferRequest`. Размер вашего предложения `offer` это то, сколько
вы предлагаете оставить другому агенту, в случае принятия предложения, ваша награда будет равна `total_amount - offer`.
- `deal_action(self, data)` возвращяет ответ (bool) согласны ли вы на предложение
от некоторого агента. Данные об агенте и о размере предложения находятся в `data`
структура класса описана здесь `base.protocol.DealRequest`
- `def round_result_action(self, data: RoundResult)` метод вызывается по окончанию раунда, собщяет общие
результаты в `data` описание `base.protocol.RoundResult`.

### Структуры данных и их поля
#### OfferRequest
```
round_id: int - номер раунда
target_agent_uid: str - идентификатор агента которому будет отосланно предложение
total_amount: int - общее количество которое необходимо разделить
```
#### DealRequest
```
round_id: int - номер раунда
from_agent_uid: str - идентификатор агента от которого поступило предложение
total_amount: int - общий размер разделяемых средств
offer: int - предложение от агента
```
#### RoundResult
```
round_id: int - номер раунда
win: bool - True если раунд успешен и предложение принято, False - в противном случае
agent_gain: Dict[str, int] - ассоциативный массив с размерами наград, ключ - идентификатор агентов раунда
disconnection_failure: bool - флаг показывает, что во время раунда произошел дисконект одного из участников
```

### Базовый тест агента

Когда вы создадите агента, пеерд сабмитом, стоит провести базовый тест.
```bash
python3 agent_test.py
```
Если вы получите - `Base test successfully completed!` значит основные требования
к методам выполненны, можете сабмитить результат.

## Сабмит агента

После того как вы создадите своего агента, нужно будет отослать результат. Текущий процесс включает в себя
использование docker образов. Если коротко, ваш агент со всеми используемыми библиотекками и файлами будет упакован
в контейнер, который будет использоваться в песочнице.

### Предварительная подготовка

1. Преждже всего вам нужно установить docker. Здесь можно найти дистрибутивы
для Mac/Win/Linux https://www.docker.com/get-started

2. После установки зайдите в терминал и наберите `docker run hello-world` если все пройдет успешно вы увидите:

```
Hello from Docker!
This message shows that your installation appears to be working correctly.
```

3. Далее вам необходимо зарегестрироваться на https://hub.docker.com/signup далее оно будет использоваться
как облачное хранилище ваших контейнеров. Запомните login и password далее они будут использоваться вам еще раз.

4. После регистрации зайдите в терминал и наберите `docker login` далее введите login и password с которым
вы рзарегестрировались на dockerhub. Если все пройдет успешно вы увидите:
```
Login Succeeded
```

### Процесс сабмита

Если базовый тест пройден, можно попробовать сделать сабмит решения. Для этого нужно упаковать решение в docker
образ и сделать push в dockerhub.

1. Сделайте `docker login` под вашим login в dockerhub далее будем обозначать его как `dockerhub_login`.
2. Создайте образ `docker build -t <dockerhub_login>/ai_agent -f Dockerfile .`
(не потеряйте точку в конце). Вместо `<dockerhub_login>` вставьте свой login от dockerhub. Обратите внимание,
образ будет сделан даже без `<dockerhub_login>`, но сделать push этого образа вы уже не сможите. В случае успеха
вы увидите в конце:
```
Successfully built <идентификатор сборки>
Successfully tagged <dockerhub_login>/ai_agent:latest
```
3. Сделайте push в dockerhub `docker push <dockerhub_login>/ai_agent` в случае успеха вы сможите зайти на dockerhub
и увидеть этот контейнер среди своих в списке.

4. Воспользуйтесь инструкцией по сабмиту решения через песочницу.

### Общие правила, рекомендации и советы
0. Версия python по умолчанию  3.7
1. Запрещяется менять client.py, делать запросы в интернет и вообще любые манипуляции с сетью или протоколом.
2. Для вашего удобства проще всего разместить весь код агента в томже файле где и основной класс и использоваьть библиотеки numpy, scikit-learn.
Однако вы можеет поместить в Docker контейнер другие необходимые файлы и библиотеки с учетом чтобы контейнер не превысил размеры в 500 Mb.
Библиотеки добавляются через requirements.txt.
4. В качестве идентификаторов агента используются строковые представления UUID. После того как подключатся все агенты,
 сервер оповестит вас о вашем uid, его можно будет узнать из поля `agent_id`. Обратите внимание, после инициализации класся,
 вызова `__init__`,  это поле будет `None` до оповещения от сервера. Однако гарантируется что `agent_id` будет доступен
 когда будут вызываться методы `offer_action`, `deal_action` и `round_result_action`.
5. Если агент не отвечает более чем 2 секунды, например долгий цикл обработки в `offer_action`, он удаляется из обработки,
никакие дальнейшие его сообщения не обрабатываются.
6. Если нужна длительная инициализация, проведите ее в методе `__init__`, так как сервер будет 1 минуту ждать инициализации
всех агентов.
7. Агент должен не упасть с ошибкой или не дисконектнуться хотя бы в 1 % раундов чтобы его очки учитывались.

Следите за обновлениями новостей на сайте и в slack, могут быть скорректированны правила или добавдленны утилиты для повышения удобстав.

## Дополнительные материалы

### Примеры агентов

В папке agent можно найти несколько примеров агентов. Ниже представленны некоторые из них.

#### Агент без памяти
Простой агент без памяти, который всегда делит поровну и принимает любое не нулевое предложение.
```python
class DummyAgent(BaseAgent):

    def offer_action(self, m):
        return m.total_amount // 2

    def deal_action(self, m):
        if m.offer > 0:
            return True
        else:
            return False

    def round_result_action(self, data):
        pass
```

### Общий алгоритм использования агента в песочнице
1. Клиент создается ваш агент через `__init__` после создания отсылает серверу сигнал готовности.
2. Сервер дожыдается некоторое время участников и начинает серию раундов.
3. Случайным образом выбираются пары агентов, один proposer другой responder.
4. Сервер спрашивает у proposer размер его предложения (вызывается метод `offer_action`) и отсылает результат
к responder.
5. У responder вызывается метод `deal_action` и получив ответ он отсылается на сервер который решает исход раунда.
6. В случае окончания раунда либо дисконекта одного из участников, сервер отсылает результат агентам и у них вызывается `round_result_action`.
7. Это повторяется много раз, пока не наберется достаточная статистика по результатам игр агентов.
8. Классы агентов все это время находятся в памяти клиентов и могу хранить состояние предыдущих раундов.

### Продвинутая сборка образов

Если вы хотите сделать сборку с классом отличным от класса по умолчанию, то это можно сделать воспользовавшись
аргементом `agent_cls_path`. Пример:
```
docker build --build-arg agent_cls_path=agent.dummy.DummyAgent -t <dockerhub_login>/ai_agent -f Dockerfile .
```

Если нужны дополнительные библиотеки в сборке, добавьте их в файл `requirements.txt` и сделайте ребилд с флагом
`--no-cache`.

Посмотреть локальные образы можно командой `docker images` удалить через `docker rmi <имя или идентификатор>`

Настройки сборки могут изменится и не все файлы из проекта могут быть включены в обоаз. Чтобы быть уверенным что файлы
попадут в образ, размещяйте их в папке agent.

Можно использовать тэги для образов подставивь в конце `:<тэг>`. Таким образом становится легче следить за версиями.
Пример:
```
docker build -t <dockerhub_login>/ai_agent:v1 -f Dockerfile .
```
При сабмите используйте адрес вместе с тэгом.
